# -*- coding: utf-8 -*-
"""Model_Training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YK0vvESGEZhjj66BYB8gGVvN39E2nF1A
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""1. Date : (DD/MM/YYYY) Day, month ('june' to 'september'), year (2012)
Weather data observations
2. Temp : temperature noon (temperature max) in Celsius degrees: 22 to 42
3. RH : Relative Humidity in %: 21 to 90
4. Ws :Wind speed in km/h: 6 to 29
5. Rain: total day in mm: 0 to 16.8
FWI Components
6. Fine Fuel Moisture Code (FFMC) index from the FWI system: 28.6 to 92.5
7. Duff Moisture Code (DMC) index from the FWI system: 1.1 to 65.9
8. Drought Code (DC) index from the FWI system: 7 to 220.4
9. Initial Spread Index (ISI) index from the FWI system: 0 to 18.5
10. Buildup Index (BUI) index from the FWI system: 1.1 to 68
11. Fire Weather Index (FWI) Index: 0 to 31.1
12. Classes: two classes, namely Fire and not Fire
"""

# Step 1: Load the data
df=pd.read_csv('/content/sample_data/Algerian_forest_fires_cleaned.csv')

df.head()

# Step 3: Feature engineering/preprocessing
df.drop(['day', 'month', 'year'], axis=1, inplace=True)

df.head()

df['Classes'].value_counts()

df['Region'].value_counts()

# Step 2: cleaning the data
df['Classes']=np.where(df['Classes'].str.contains("not fire"),0,1)

df.tail()

df['Classes'].value_counts()

# step 4 Data Exploration/Visualizations
# Univariate, BiVariate & Multi Variate

# Univariate Analysis
plt.style.use('seaborn')
df.hist(bins=30, figsize=(20,15))
plt.show()

plt.figure (figsize=(15,10))
sns.heatmap(df.corr(), annot=True)

# Feature Engineering
# Multi COllinearity (All IV should be independent to each other)

# Going to drop 'DC' & 'BUI' from df because of multicollinearity with 'DMC'
df.drop(['DC', 'BUI'], axis=1, inplace=True)

df.head()

# Step 6 Split IV and DV from dataset.

y=df['FWI']

X=df.drop(['FWI'], axis=1,)

X.shape, y.shape

X.head()

y

from sklearn.model_selection import train_test_split

# step 7 split the data into two parts Train & test
X_train, X_test, y_train, y_test=train_test_split(X,y, test_size=0.20, random_state=42)

X_train.shape, X_test.shape, y_train.shape, y_test.shape

# step 8 Scaling the data
from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()

X_train_sc=scaler.fit_transform(X_train)
X_test_sc=scaler.transform(X_test)

X_train

X_train_sc

plt.subplots(figsize=(15, 5))
plt.subplot(1, 2, 1)
sns.boxplot(data=X_train)
plt.title('X_train Before Scaling')
plt.subplot(1, 2, 2)
sns.boxplot(data=X_train_sc)
plt.title('X_train After Scaling')

# step 9 is called Model Training
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error,r2_score,mean_squared_error

lin_reg=LinearRegression()

# Train my model with scaled training data
lin_reg.fit(X_train_sc, y_train)

lin_reg.coef_

lin_reg.intercept_

# 1. Linear_Reg
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
linreg=LinearRegression()
linreg.fit(X_train_sc,y_train)
y_pred=linreg.predict(X_test_sc)
mae=mean_absolute_error(y_test,y_pred)
score=r2_score(y_test,y_pred)
print("Mean absolute error", mae)
print("R2 Score", score)
plt.scatter(y_test,y_pred)

# 2. lasso Regression
from sklearn.linear_model import Lasso
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
lasso=Lasso()
lasso.fit(X_train_sc,y_train)
y_pred=lasso.predict(X_test_sc)
mae=mean_absolute_error(y_test,y_pred)
score=r2_score(y_test,y_pred)
print("Mean absolute error", mae)
print("R2 Score", score)
plt.scatter(y_test,y_pred)

# 3 Lasso with CV
from sklearn.linear_model import LassoCV
lassocv=LassoCV(cv=5)
lassocv.fit(X_train_sc,y_train)
y_pred=lassocv.predict(X_test_sc)
plt.scatter(y_test,y_pred)
mae=mean_absolute_error(y_test,y_pred)
score=r2_score(y_test,y_pred)
print("Mean absolute error", mae)
print("R2 Score", score)

# 4 Ridge Regression
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
ridge=Ridge()
ridge.fit(X_train_sc,y_train)
y_pred=ridge.predict(X_test_sc)
mae=mean_absolute_error(y_test,y_pred)
score=r2_score(y_test,y_pred)
print("Mean absolute error", mae)
print("R2 Score", score)
plt.scatter(y_test,y_pred)

# 5 Ridge Regression with Cv
from sklearn.linear_model import RidgeCV
ridgecv=RidgeCV(cv=5)
ridgecv.fit(X_train_sc,y_train)
y_pred=ridgecv.predict(X_test_sc)
plt.scatter(y_test,y_pred)
mae=mean_absolute_error(y_test,y_pred)
score=r2_score(y_test,y_pred)
print("Mean absolute error", mae)
print("R2 Score", score)

# 6 Elastic_net Regression
from sklearn.linear_model import ElasticNet
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
elastic=ElasticNet()
elastic.fit(X_train_sc,y_train)
y_pred=elastic.predict(X_test_sc)
mae=mean_absolute_error(y_test,y_pred)
score=r2_score(y_test,y_pred)
print("Mean absolute error", mae)
print("R2 Score", score)
plt.scatter(y_test,y_pred)

# 7 Elastic_net Regression with Cv
from sklearn.linear_model import ElasticNetCV
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
elasticcv=ElasticNetCV(cv=5)
elasticcv.fit(X_train_sc,y_train)
y_pred=elastic.predict(X_test_sc)
mae=mean_absolute_error(y_test,y_pred)
score=r2_score(y_test,y_pred)
print("Mean absolute error", mae)
print("R2 Score", score)
plt.scatter(y_test,y_pred)

"""# Best Fit line Equation

Predicted_FWI= -0.026 * Temp - 0.18 * RH - 0.014 * Ws - 0.033 * Rain - 0.824 * FFMC + 3.81 * DMC + 4.89 * ISI + 0.39 * Classes - 0.40 * Region + 7.42
"""

# step 11 To write the pickle files for the future IV prediction
import pickle
pickle.dump(scaler,open('scaler.pkl', 'wb'))
pickle.dump(lin_reg,open('lin_reg.pkl', 'wb'))

X.describe()